\chapter{Introduction}

It has those serifs because angles, compared to curves, require less bytes to be stored on a hard drive, and that was a problem in the time Charter was made. Hence, Charter was designed for low-resolution printing, and it works well in that environment, but if you print in high-resolution or with a type size of 12+pt it may look a bit "blunt".
On the other side it is an economical (in terms of space) and serious typeface, very readable and with nice proportions, having a large but not bloated x-height.

\begin{definition}[Vector space]
A \defn{vector space} over a field $\mathcal{F}$ consists of a set $\mathcal{V}$ and two binary operations: \defn{vector addition} (denoted by $\mathbf{u} + \mathbf{v}$ for $\mathbf{u}, \mathbf{v} \in \mathcal{V}$) and \defn{scalar multiplication} (denoted by $\alpha \mathbf{v}$ for $\alpha \in \mathcal{F}, \mathbf{v} \in \mathcal{V}$). These two operations must satisfy the following axioms:

%\begin{align}
%	\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w} \\ \eqname{Associativity of addition} \\
%    \mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u} \\ \eqname{Commutativity of addition} \\
%\end{align}

\begin{axiom}[Associativity of addition\nopunct]
	\begin{equation}
		\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}
    \end{equation}
\end{axiom}

\begin{axiom}[Commutativity of addition]
    \begin{equation}
    	\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}
    \end{equation}
\end{axiom}

\end{definition}

\begin{definition}[Inner product space]
A inner product space is a vector space with an extra operation called the \defn{inner product}: $\innerProd{\cdot, \cdot} : \mathcal{V} \times \mathcal{V} \to \mathcal{F}$ that satisfies the following axioms:
\end{definition}

\begin{definition}[Gram matrix]
Given a kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ and observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$, the $n \times n$ real matrix $K$ with elements
\begin{equation}
	K_{ij} \defeq k(\mathbf{x}_i, \mathbf{x}_j)
\end{equation}
for $1 \leq i, j \leq n$ is called the \defn{Gram matrix} of $k$ with respect to $\mathbf{x}_1, \dotsc, \mathbf{x}_n$.
\end{definition}

\begin{definition}[Positive semi-definite matrix]
A real symmetric $n \times n$ matrix $K$ satisfying
\begin{equation}
	\sum_{i, j=1}^n c_i c_j K_{ij} \geq 0
\end{equation}
for all $c_i, c_j \in \mathbb{R}$ is called \defn{positive semi-definite}. If the term on the left is strictly positive, then $K$ is called \defn{positive definite}.
\end{definition}

\begin{definition}[Positive definite kernel]
Let $\mathcal{X}$ be a non-empty set. A symmetric function $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \defn{positive definite kernel} (or simply a \defn{kernel}) if the Gram matrix of $k$ with respect to all finite sized observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$ ($n \in \mathbb{N}$) is positive semi-definite.
\end{definition}

\begin{theorem}[Mercer's theorem]
If $k$ is a kernel on $\mathcal{X} \times \mathcal{X}$, then it can be written in the form:
\begin{equation}
	k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\phi(\mathbf{x}),  \phi(\mathbf{x}^\prime)}_\mathcal{V}
\end{equation}
for some inner product space $\mathcal{V}$.
\end{theorem}

The function $\phi : \mathcal{X} \to \mathcal{V}$ is a non-linear map from the input space $\mathcal{X}$ to an inner product space $\mathcal{V}$ called the \defn{feature space}. Hence, we can view kernels as a way of computing the inner product in a feature space without explicitly mapping between input space and feature space. This technique is known as the \defn{kernel trick}, which is commonly used in non-linear support vector machines.

As an example, consider the kernel $k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\mathbf{x}, \mathbf{x}^\prime}^2$. If we $\mathbf{x}, \mathbf{x}^\prime \in \mathbb{R}^2$, then $k((x_1, x_2)^\intercal, (x_1^\prime, x_2^\prime)^\intercal)$ can be written as

\begin{equation}
\innerProd{(x_1^2, x_2^2, \sqrt{2} x_1 x_2)^\intercal, (x_1^{\prime 2}, x_2^{\prime 2}, \sqrt{2} x_1^\prime x_2^\prime)^\intercal}
\end{equation}

Note that this decomposition is not unique, since we can also write

\begin{equation}
2
\end{equation}

However, there is a special feature space that is unique to a given kernel. 

Let $\mathcal{F}$ be the space of functions mapping $\mathcal{X}$ to $\mathbb{R}$. We use the feature map $\phi(x) : \mathcal{X} \to \mathcal{F}$:
\begin{equation}
	\phi(x) = k(x, \cdot)
\end{equation}
It is important to note here that $\phi(x)$ is a \emph{function}. It takes an element $x^\prime \in \mathcal{X}$ and maps it to the real number $k(x, x^\prime)$. In an abuse of notation, we can equivalently write $\phi(x)(x^\prime) = k(x, x^\prime)$.


\begin{equation}
	f(\cdot) = \sum_{i=1}^n \alpha_i k(x_i, \cdot)
\end{equation}

\begin{equation}
	\innerProd{f, g} \defeq \sum_{i=1}^n \sum_{j=1}^{n^\prime} \alpha_i \beta_j k(x_i, x_j^\prime)
\end{equation}

\begin{definition}[Reproducing kernel]
	A kernel $k$ is a \defn{reproducing kernel} of a Hilbert space $\mathcal{H}$ if $\forall f \in \mathcal{H}, f(x) = \innerProd{k(x, \cdot), f}_\mathcal{H}$.
\end{definition}

\begin{theorem}[Moore-Aronszajn theorem]
Suppose $k(\cdot, \cdot)$ is a symmetric, positive definite kernel on a set $\mathcal{X}$. Then there is a unique Hilbert space of functions on $\mathcal{X}$ for which $k$ is a reproducing kernel.
\end{theorem}


Let $X$ be a random variable with codomain $\Omega$ and probability distribution $P(X)$. Furthermore, let $k$ be a kernel on $\Omega \times \Omega$ and $\mathcal{H}$ be the corresponding RKHS induced by the kernel according to the Moore-Aronszajn Theorem.

Using the reproducing property of $\mathcal{H}$, we can express the inner product of two elements in the RKHS $k(x, \cdot)$ and $k(x^\prime, \cdot)$ as  we obtain a feature map:

\begin{align}
	\innerProd{k(x, \cdot), k(x^\prime, \cdot)} &= k(x, x^\prime)
    \innerProd{\ph}
\end{align}

We use the feature map $\phi(X)$

\begin{definition}[Mean map]
The embedding of $P(X)$ in $\mathcal{H}$ via the \defn{mean map} is given by
\begin{equation}
	\mu_{X}\nomenclature{$\mu_X$}{Embedding of $X$ in a RKHS via the mean-map} \defeq \E[k(X, \cdot)] = \E[\phi(X)]
\end{equation}
\end{definition}

\begin{equation}
\mu_{X} = \int_\Omega \phi(x) \ \mathrm{d}P(x)
\end{equation}

\begin{equation}
\mu_{X} = \frac{1}{n} \sum_{i=1}^n \phi(x_i)
\end{equation}

\begin{equation}
\E_X[f(X)] = \innerProd{f, \mu_X}_\mathcal{H}
\end{equation}

\begin{equation}
k(x,x') = \exp\left(-\frac{1}{2\sigma^2} \normLL{x-x'}^2 \right)  
\end{equation}

\begin{equation}
	C_{XY} = \E_{X, Y}[\phi(X) \otimes \phi(Y)]
\end{equation}

\begin{equation}
	\widehat{C}_{XY} = \frac{1}{n} \sum_{i=1}^n \phi(x_i) \otimes \phi(y_i)
\end{equation}

\begin{equation}
	C_{Y \mid X} = C_{YX} C_{XX}^{-1}
\end{equation}

