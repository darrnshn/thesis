\chapter{Background}
This chapter provides the mathematical background for this work. We begin with some motivation for the theory. Suppose we are working with some abstract set $\mathcal{S}$ (for instance, a set of probability distributions). We can either:

\begin{enumerate}
  \item Develop the necessary theory to work directly with $S$.
  \item Map elements of $S$ to another set $S^\prime$ and work within $S^\prime$ instead.
\end{enumerate}

The latter choice is advantageous if the new set $S^\prime$ is more convenient to work with. If we do this, we can exploit properties, theorems, computational savings, and intuition we have for $S^\prime$ to reason about our original set $S$. All we need is an ``embedding'' or a ``representation theorem'' to transform elements of $S$ to $S^\prime$.

This approach underpins the theory in this thesis. Instead of directly working with probability distributions, we embed them in a Hilbert space. Mapping to a Hilbert space has several benefits:

\begin{itemize}
	\item Hilbert spaces are a generalisation of the Euclidean space, so we can use our geometric intuition to .
    \item They are frequently used in mathematics and physics/powerful.
    \item Partially due to its intuitive basis on Euclidean space, they ... 
\end{itemize}

\todo{Merge with the introduction metadiscourse?}

\todo{Do we begin with fields?}

\section{Vector Spaces}
We begin with \defn{vector spaces}. A vector space, loosely speaking, is a set of objects (called \defn{vectors}) that can be added together and multiplied by numbers (called \defn{scalars}). More formally,

\begin{definition}[Vector space]
A \defn{vector space} over a field $\mathbb{F}$ consists of a set $\mathcal{V}$, a \defn{vector addition} operation $\oplus$, and a \defn{scalar multiplication} operation $\otimes$. It must obey the following axioms for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$ and $a, b \in \mathbb{F}$:
\begin{description}
  \item[Associativity] $\mathbf{u} \oplus (\mathbf{v} \oplus \mathbf{w}) = (\mathbf{u} \oplus \mathbf{v}) \oplus \mathbf{w}$
  \item[Commutativity] $\mathbf{u} \oplus \mathbf{v} = \mathbf{v} \oplus \mathbf{w}$ and $a \otimes \mathbf{u} = \mathbf{u} \otimes a$
  \item[Existence of Identity] There exists elements $\mathbf{0} \in \mathcal{V}$ and $1 \in \mathbb{F}$ such that $\mathbf{0} \oplus \mathbf{u} = \mathbf{u}$ and $1 \otimes \mathbf{u} = \mathbf{u}$.
  \item[Existence of Inverse] For every element $\mathbf{u}$, there exists an element $-\mathbf{u} \in \mathcal{V}$ such that $\mathbf{u} \oplus (-\mathbf{u}) = \mathbf{0}$.
  \item[Distributivity] $a \otimes (\mathbf{u} \oplus \mathbf{v}) = (a \otimes \mathbf{u}) \oplus (a \otimes \mathbf{v})$ and $(a + b) \otimes \mathbf{u} = (a \otimes \mathbf{u}) \oplus (b \cdot \mathbf{v})$
\end{description}
\end{definition}

While the canonical example of a vector space is the space of real n-dimensional vectors $\mathbb{R}^n$, this definition is general enough to define vector spaces of other mathematical objects. For instance, $\mathbb{R}^\mathbb{R}$, the set of functions from $\mathbb{R}$ to $\mathbb{R}$, is a vector space. In this space, vector addition and scalar multiplication can be defined as:

\begin{align}
  (f \oplus g)(x) &\defeq f(x) g(x) \\
  (a \otimes f)(x) &\defeq a f(x)
\end{align}

for $f, g \in \mathbb{R}^\mathbb{R}$ and $a \in \mathbb{R}$.

Starting with a vector space, we can add additional operations to it to form richer structures:

\todo{Should we work with inner product spaces or just inner products?}

\begin{definition}[Inner product space]
An \defn{inner product space} is a vector space $\mathcal{V}$ over $\mathbb{F}$ endowed with a map $\innerProd{\cdot, \cdot}_\mathcal{V} : \mathcal{V} \times \mathcal{V} \to \mathbb{F}$ that satisfies the following axioms for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$ and $a \in \mathbb{F}$:
\begin{description}
  \item[Linearity] $\innerProd{a \otimes \mathbf{v}, \mathbf{w}}_\mathcal{V} = a \innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{V}$ and $\innerProd{\mathbf{u} \oplus \mathbf{v}, \mathbf{w}}_\mathcal{V} = \innerProd{\mathbf{u}, \mathbf{w}}_\mathcal{V} + \innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{V}$.
  \item[Positive-definiteness] $\innerProd{\mathbf{v}, \mathbf{v}}_\mathcal{V} \geq 0$ and $\innerProd{\mathbf{v}, \mathbf{v}}_\mathcal{V} = 0 \Leftrightarrow \mathbf{v} = \mathbf{0}$.
  \item[Conjugate symmetry] $\innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{V} = \innerProd{\mathbf{w}, \mathbf{v}}_\mathcal{V}$.
\end{description}
\end{definition}

For example, we can extend the space $\mathbb{R}^n$ with the dot product to form a inner product space:

\begin{equation}
  \innerProd{\mathbf{x}, \mathbf{y}}_{\mathbb{R}^n} \defeq \mathbf{x}^\intercal \mathbf{y} = \sum_{i=1}^n x_i y_i
\end{equation}

\todo{mention how inner products can be seen as similarity?}

\section{Norms}
Another useful operation to have is the ability to find the ``size'' of a vector. We can do this by adding a \defn{norm} to a vector space to form a \defn{normed vector space}.
\begin{definition}[Norm]
Given a vector space $\mathcal{V}$ over $\mathbb{R}$, a \defn{norm} on $\mathcal{V}$ is a function $\normLL{\cdot} : \mathcal{V} \to \mathbb{R}$ that satisfies the following axioms for all $\mathbf{u}, \mathbf{v} \in \mathcal{V}$ and $a \in \mathbb{R}$:
\begin{description}
  \item[Non-negativity] $\normLL{\mathbf{v}} \geq 0$ and $\normLL{\mathbf{v}} = 0 \Leftrightarrow \mathbf{v} = \mathbf{0}$.
  \item[Homogeneity] $\normLL{a \mathbf{v}} = \normL{a} \normLL{\mathbf{v}}$.
  \item[Positive-definiteness] $\normLL{\mathbf{u} + \mathbf{v}} \leq \normLL{\mathbf{u}} + \normLL{\mathbf{v}}$.
\end{description}
\end{definition}

\todo{Give another example for functions?}

Given an inner product space $\mathcal{V}$ over $\mathbb{R}$ with inner product $\innerProd{\cdot, \cdot}_\mathcal{V} : \mathcal{V} \times \mathcal{V} \to \mathbb{R}$, a natural norm to define for any vector $\mathbf{u} \in \mathcal{V}$ is:
\begin{equation}
	\normLL{\mathbf{u}} \defeq \sqrt{\innerProd{\mathbf{u}, \mathbf{u}}_\mathbf{V}} 
\end{equation}

This is the most common norm for a general inner product space, but other norms can be defined. \todo{Give example?}

An interesting consequence of using this particular norm is that we can express the \defn{distance} between two vectors in terms of this norm.

\begin{definition}[Distance ]
\end{definition}

\section{Complete Vector Spaces}

\section{Hilbert Spaces}

\begin{definition}[Hilbert space]
A Hilbert
\end{definition}


\section{Kernels}

\begin{definition}[Gram matrix]
Given a kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ and observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$, the $n \times n$ real matrix $K$ with elements
\begin{equation}
	K_{ij} \defeq k(\mathbf{x}_i, \mathbf{x}_j)
\end{equation}
for $1 \leq i, j \leq n$ is called the \defn{Gram matrix} of $k$ with respect to $\mathbf{x}_1, \dotsc, \mathbf{x}_n$.
\end{definition}

\begin{definition}[Positive semi-definite matrix]
A real symmetric $n \times n$ matrix $K$ satisfying
\begin{equation}
	\sum_{i, j=1}^n c_i c_j K_{ij} \geq 0
\end{equation}
for all $c_i, c_j \in \mathbb{R}$ is called \defn{positive semi-definite}. If the term on the left is strictly positive, then $K$ is called \defn{positive definite}.
\end{definition}

\begin{definition}[Positive definite kernel]
Let $\mathcal{X}$ be a non-empty set. A symmetric function $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \defn{positive definite kernel} (or simply a \defn{kernel}) if the Gram matrix of $k$ with respect to all finite sized observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$ ($n \in \mathbb{N}$) is positive semi-definite.
\end{definition}

\begin{theorem}[Mercer's theorem]
If $k$ is a kernel on $\mathcal{X} \times \mathcal{X}$, then it can be written in the form:
\begin{equation}
  k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\phi(\mathbf{x}), \phi(\mathbf{x}^\prime)}_\mathcal{V}
\end{equation}
for some inner product space $\mathcal{V}$.
\end{theorem}

The function $\phi : \mathcal{X} \to \mathcal{V}$ is an implicit \defn{feature map} from the input space $\mathcal{X}$ to an inner product space $\mathcal{V}$ called the \defn{feature space}. Mercer's theorem shows that kernels can be thought as way of computing the inner product in some feature space without explicitly mapping between input space and feature space. This technique is known as the \defn{kernel trick}, which is commonly used in non-linear support vector machines.

This is significant. Since , kernels 

As an example, consider the kernel $k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\mathbf{x}, \mathbf{x}^\prime}^2$. If we $\mathbf{x}, \mathbf{x}^\prime \in \mathbb{R}^2$, then $k((x_1, x_2)^\intercal, (x_1^\prime, x_2^\prime)^\intercal)$ can be written as

\begin{equation}
\innerProd{(x_1^2, x_2^2, \sqrt{2} x_1 x_2)^\intercal, (x_1^{\prime 2}, x_2^{\prime 2}, \sqrt{2} x_1^\prime x_2^\prime)^\intercal}
\end{equation}

Note that this decomposition is not unique, since we can also write

\begin{equation}
2
\end{equation}

\section{Reproducing Kernel Hilbert Spaces}
However, there is a special feature space that is unique to a given kernel. 

Let $\mathbb{F}$ be the space of functions mapping $\mathcal{X}$ to $\mathbb{R}$. We use the feature map $\phi(x) : \mathcal{X} \to \mathbb{F}$:
\begin{equation}
	\phi(x) = k(x, \cdot)
\end{equation}
It is important to note here that $\phi(x)$ is a \emph{function}. It takes an element $x^\prime \in \mathcal{X}$ and maps it to the real number $k(x, x^\prime)$. In an abuse of notation, we can equivalently write $\phi(x)(x^\prime) = k(x, x^\prime)$.

\begin{equation}
	f(\cdot) = \sum_{i=1}^n \alpha_i k(x_i, \cdot)
\end{equation}

\begin{equation}
	\innerProd{f, g} \defeq \sum_{i=1}^n \sum_{j=1}^{n^\prime} \alpha_i \beta_j k(x_i, x_j^\prime)
\end{equation}

\begin{definition}[Reproducing kernel]
	A kernel $k$ is a \defn{reproducing kernel} of a Hilbert space $\mathcal{H}$ if $\forall f \in \mathcal{H}, f(x) = \innerProd{k(x, \cdot), f}_\mathcal{H}$.
\end{definition}

Using the reproducing property of $\mathcal{H}$, we can express the inner product of two elements in the RKHS $k(x, \cdot)$ and $k(x^\prime, \cdot)$ as we obtain a feature map:

\begin{align}
	\innerProd{k(x, \cdot), k(x^\prime, \cdot)} &= k(x, x^\prime)
\end{align}

We use the feature map $\phi(X)$



\begin{theorem}[Moore-Aronszajn theorem]
Suppose $k(\cdot, \cdot)$ is a symmetric, positive definite kernel on a set $\mathcal{X}$. Then there is a unique Hilbert space $\mathcal{H}$ of functions on $\mathcal{X}$ for which $k$ is a reproducing kernel.
\end{theorem}

Let $X$ be a random variable with codomain $\Omega$ and probability distribution $P(X)$. Also let $k$ be a kernel on $\Omega \times \Omega$. From the Moore-Aronszajn Theorem, we know there exists an unique RKHS $\mathcal{H}$ where $k$ is a reproducing kernel. Furthermore, we can define the implicit feature map $\phi(x) \defeq k(x, \cdot)$ from $\Omega$ to $\mathcal{H}$ so that $k(x, x^\prime) = \innerProd{phi(x), phi(x)^\prime}$.

\section{Kernel Embedding of Distributions}

\begin{definition}[Mean embedding]
The embedding of $P(X)$ in $\mathcal{H}$ via the \defn{mean embedding} is given by
\begin{equation}
  \mu_{X}\nomenclature{$\mu_X$}{Embedding of $X$ in a RKHS via the mean-map} \defeq \E[k(X, \cdot)] = \E[\phi(X)]
\end{equation}
\end{definition}

In other words, the mean embedding is the expected value of the feature map of the kernel under the distribution $P(X)$.

\begin{equation}
\mu_{X} = \int_\Omega \phi(x) \ \mathrm{d}P(x)
\end{equation}

In the common case that we don't know the true distribution, we can use Equation to empirically estimate the mean embedding from samples drawn I.I.D. from $P(X)$.

\begin{equation}
\mu_{X} = \frac{1}{n} \sum_{i=1}^n \phi(x_i)
\end{equation}

The mean embedding has several advantages. Firstly, by the reproducing property of $\mathcal{H}$, the expectation of any function $f \in \mathcal{H}$ can be found by taking its inner product with the kernel embedding:

\begin{equation}
\E_X[f(X)] = \innerProd{f, \mu_X}_\mathcal{H}
\end{equation}

\begin{definition}[Characteristic kernel]
  Let If the mean ifmap of a kernel is injective, then the kernel is called \defn{characteristic}.
\end{definition}

So if the kernel we use is characteristic, then any mean embedding in the RKHS retains all the statistical features of the distribution. In other words, no information is lost from this transformation.

\begin{equation}
  k(x,x') = \exp\left(-\frac{1}{2\sigma^2} \normLL{x-x'}^2 \right)
\end{equation}

An important concept t

\begin{equation}
  C_{XY} = \E_{X, Y}[\phi(X) \otimes \phi(Y)]
\end{equation}

\begin{equation}
	\widehat{C}_{XY} = \frac{1}{n} \sum_{i=1}^n \phi(x_i) \otimes \phi(y_i)
\end{equation}

\begin{equation}
	C_{Y \mid X} = C_{YX} C_{XX}^{-1}
\end{equation}
