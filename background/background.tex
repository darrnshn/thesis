\chapter{Background}
This chapter introduces the mathematical background behind this work. The mathematical theory is motivated by the technique of solving a problem by transforming it into another. Suppose we are working with an abstract set $\mathcal{S}$ (e.g. a set of probability distributions). Instead of working with $\mathcal{S}$ directly, we could map elements of $\mathcal{S}$ to another set $\mathcal{S}^\prime$ and work entirely in $\mathcal{S}^\prime$. If the new set is easier to work with, then this approach could be better than working with the original set. All we need is an ``embedding'' to transform elements of $S$ to $S^\prime$.

The \todo{approach?} is \todo{inspired?} by this technique. Instead of working with probability distributions directly, we embed them in a reproducing kernel Hilbert space (RKHS) and work entirely in the Hilbert space instead. Using a Hilbert space has several benefits:
%
\begin{itemize}
  \item Hilbert spaces generalise Euclidean space, so we can apply our geometric intuition to elements of the space.
  \item They have been successfully used in this way for partial differential equations, quantum mechanics, and many other areas \needcite.
  \item They have a relatively simple structure compared to other topological vector spaces \needcite.
\end{itemize}
%
Our discussion begins with vector spaces, which are algebraic structures that we can extend to a Hilbert space. \todo{We then introduce reproducing kernel Hilbert spaces and positive-definite kernels. Finally, we show how probability distributions can be embedded in a RKHS.}

\section{Vector Spaces}
A vector space, loosely speaking, is a set of objects (called \defn{vectors}) that can be added together and multiplied by numbers (called \defn{scalars}). More formally,

\begin{definition}[Vector space]
A \defn{vector space} over a field\footnote{Informally, a field is a set of elements where addition and multiplication of these elements satisfy several axioms such as associativity and closure. An example of a field is the set of real numbers $\mathbb{R}$.} $\mathbb{F}$ consists of a set $\mathcal{V}$, a \defn{vector addition} operation $\oplus$, and a \defn{scalar multiplication} operation $\otimes$. It must obey the following axioms for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$ and $a, b \in \mathbb{F}$:
%
\begin{description}
  \item[Associativity] $\mathbf{u} \oplus (\mathbf{v} \oplus \mathbf{w}) = (\mathbf{u} \oplus \mathbf{v}) \oplus \mathbf{w}$.
  \item[Commutativity] $\mathbf{u} \oplus \mathbf{v} = \mathbf{v} \oplus \mathbf{w}$ and $a \otimes \mathbf{u} = \mathbf{u} \otimes a$.
  \item[Existence of Identity] There exists elements $\mathbf{0} \in \mathcal{V}$ and $1 \in \mathbb{F}$ such that $\mathbf{0} \oplus \mathbf{u} = \mathbf{u}$ and $1 \otimes \mathbf{u} = \mathbf{u}$.
  \item[Existence of Inverse] For every $\mathbf{u}$, there is an element $-\mathbf{u} \in \mathcal{V}$ such that $\mathbf{u} \oplus (-\mathbf{u}) = \mathbf{0}$.
  \item[Distributivity] $a \otimes (\mathbf{u} \oplus \mathbf{v}) = (a \otimes \mathbf{u}) \oplus (a \otimes \mathbf{v})$ and $(a + b) \otimes \mathbf{u} = (a \otimes \mathbf{u}) \oplus (b \cdot \mathbf{v})$
\end{description}
\end{definition}
%
While the canonical example of a vector space is the space of real n-dimensional vectors $\mathbb{R}^n$, we can also define vector spaces of other mathematical objects. For instance, $\mathbb{R}^\mathbb{R}$, the set of functions from $\mathbb{R}$ to $\mathbb{R}$, is a vector space. In this space, vector addition and scalar multiplication can be defined as:
%
\begin{align}
  (f \oplus g)(x) &\defeq f(x) g(x) \\
  (a \otimes f)(x) &\defeq a f(x)
\end{align}
%
for any $f, g \in \mathbb{R}^\mathbb{R}$ and $a \in \mathbb{R}$.

\section{Banach Spaces}
One nice property to have in our vector space is the notion of \defn{completeness}. Intuitively, a complete vector space has no ``holes'' in it. For example, the interval $(0, 1]$ (i.e. the set of real numbers $0 < x \leq 1$) is not complete since $0$ is not in the set, even though the set contains elements arbitrarily close to $0$.

We can capture this intuition about completeness formally. Given a vector space $\mathcal{V}$ over $\mathbb{R}$, let $d : \mathcal{V} \times \mathcal{V} \to \mathbb{R}$ be a \defn{distance function} that satisfies the following axioms for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$:
%
\begin{description}
  \item[Positive-definiteness] $d(\mathbf{u}, \mathbf{v}) \geq 0$ and $d(\mathbf{u}, \mathbf{v}) = 0 \Leftrightarrow \mathbf{u} = \mathbf{v}$.
  \item[Symmetry] $d(\mathbf{u}, \mathbf{v}) = d(\mathbf{v}, \mathbf{u})$.
  \item[Triangular inequality] $d(\mathbf{u}, \mathbf{w}) \leq d(\mathbf{u}, \mathbf{v}) + d(\mathbf{v}, \mathbf{w})$.
\end{description}
%
Given a distance function $d$, a sequence $\langle x_i \rangle_{i=1}^\infty$ is \defn{Cauchy} if, for every real number $\epsilon > 0$, there is a positive integer $N$ such that for all positive integers $m, n \geq N$, we have $d(x_m, x_n) < \epsilon$. In other words, members of the sequence get closer and closer to each other with respect to $d$ until they converge to a limit.

A vector space $\mathcal{V}$ is \defn{complete} if every Cauchy sequence of vectors in $\mathcal{V}$ converges to a limit also in $\mathcal{V}$. Continuing our example for $(0, 1]$, the sequence $x_i = 1 / i$ is Cauchy and converges to $0$, which is not in the interval. Therefore $(0, 1]$ is not complete.

We can now extend a vector space with this notion of completeness:
%
\begin{definition}[Banach space]
A \defn{Banach} space is vector space that is complete with respect to the distance function:
\begin{equation}
  d(\mathbf{u}, \mathbf{v}) = \normLL{\mathbf{u} - \mathbf{v}}
\end{equation}
where $\normLL{\cdot}$ is a function from $\mathcal{V}$ to $\mathbb{R}$ (called the \defn{norm}) that satisfies the following axioms for all $\mathbf{u}, \mathbf{v} \in \mathcal{V}$ and $a \in \mathbb{R}$:
\begin{description}
  \item[Positive-definiteness] $\normLL{\mathbf{u}} \geq 0$ and $\normLL{\mathbf{u}} = 0 \Leftrightarrow \mathbf{u} = \mathbf{0}$.
  \item[Homogeneity] $\normLL{a \mathbf{u}} = \normL{a} \normLL{\mathbf{u}}$.
  \item[Triangle Inequality] $\normLL{\mathbf{u} + \mathbf{v}} \leq \normLL{\mathbf{u}} + \normLL{\mathbf{v}}$.
\end{description}
\end{definition}

An example of a Banach space is $\mathbb{R}^n$. There are actually an infinite number of norms that can be defined on $\mathbb{R}^n$, with the most common being the L2 norm $\normLL{\mathbf{u}}_2 \defeq \sqrt{\sum_{i=1}^n u_i}$ for a vector $\mathbf{u} \in \mathcal{H}$. 

\section{Hilbert Spaces}

\begin{definition}[Hilbert space]
A \defn{Hilbert space} $\mathcal{H}$ over $\mathbb{F}$ is a Banach space endowed with a inner product operation $\innerProd{\cdot, \cdot}_\mathcal{H} : \mathcal{H} \times \mathcal{H} \to \mathbb{F}$ satisfying the following axioms for all $\mathbf{u}, \mathbf{v} \in \mathcal{H}$ and $a \in \mathbb{F}$:
%
\begin{description}
  \item[Linearity] $\innerProd{a \mathbf{v}, \mathbf{w}}_\mathcal{H} = a \innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{H}$ and $\innerProd{\mathbf{u} \oplus \mathbf{v}, \mathbf{w}}_\mathcal{H} = \innerProd{\mathbf{u}, \mathbf{w}}_\mathcal{H} + \innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{H}$.
  \item[Positive-definiteness] $\innerProd{\mathbf{v}, \mathbf{v}}_\mathcal{H} \geq 0$ and $\innerProd{\mathbf{v}, \mathbf{v}}_\mathcal{H} = 0 \Leftrightarrow \mathbf{v} = \mathbf{0}$.
  \item[Ssymmetry] $\innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{H} = \innerProd{\mathbf{w}, \mathbf{v}}_\mathcal{H}$.
\end{description}
%
where the norm for the Banach space is given by:
%
\begin{equation}
  \normLL{\mathbf{u}}_\mathcal{H} = \sqrt{\innerProd{\mathbf{u}, \mathbf{u}}_\mathcal{H}}
\end{equation}
%
\end{definition}

As we mentioned in the start of this chapter, Hilbert spaces are a generalisation of Euclidean space, so it should not be surprising that $\mathbb{R}^n$ is a Hilbert space, with the inner product given by $\innerProd{\mathbf{u}, \mathbf{v}}_\mathcal{H} \defeq \sum_{i=0}^n u_i v_i$ for vectors $\mathbf{u}, \mathbf{v} \in \mathcal{H}$.

\section{Reproducing Kernel Hilbert Spaces}
We define a reproducing kernel Hilbert space (RKHS) to be a Hilbert space of functions whose elements satisfy a useful property\footnote{A more standard and formal definition is to use bounded linear functionals, but our definition does not require familarity with functional analysis. \needcite}:
%
\begin{definition}[Reproducing Kernel Hilbert Space]
A Hilbert space $\mathcal{H}$ of real-valued functions on $\mathcal{X}$ is a \defn{reproducing kernel Hilbert space (RKHS)} if, for every $x \in \mathcal{X}$, there is unique function $K_x \in \mathcal{H}$ (called the \defn{representer} of $x$) with the reproducing property,
\begin{equation}
  f(x) = \innerProd{f, K_x}_\mathcal{H} \quad \forall f \in \mathcal{H}
\end{equation}
\end{definition}

So in a RKHS $\mathcal{H}$, evaluating a function $f \in \mathcal{H}$ at a point $x \in \mathcal{X}$ is equivalent to taking the inner product of $f$ with the representer of $x$. Using this reproducing property, we can then define the \defn{reproducing kernel} of $\mathcal{H}$ as $K(x, y) \defeq K_x(y) = \innerProd{K_x, K_y}_\mathcal{H}$.

\subsection{Positive Definite Kernels}
The reproducing kernels in the previous section belong to a special class of functions called \defn{positive definite kernels}:
%
\begin{definition}
Let $\mathcal{X}$ be a nonempty set. A symmetric function $K : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \defn{positive definite kernel} (or just a \defn{kernel}) on $\mathcal{X}$ if it satisfies:
\begin{equation}
	\sum_{i,j=1}^n c_i c_j K(x_i, x_j) \geq 0
\end{equation}
for any $n \in \mathbb{N}$, $x_1, \dots, x_n \in \mathcal{X}$, and $c_1, \dots, c_n \in \mathbb{R}$.
\end{definition}
%
Since $\sum_{i,j=1}^n c_i c_j K(x, x^\prime) = \innerProd{\sum_{i=1}^n c_i K_x, \sum_{j=1}^n c_j K_{x^\prime}} \geq 0$ by the reproducing property, we can see that all reproducing kernels are positive definite.

There are many examples of positive definite kernels, such as linear kernels, polynomial kernels, and Gaussian kernels \todo{give explicit formulas}?.

\subsection{Constructing RKHSs}
\todo{For any of this to be useful, we need to specify what sort of RKHS we are mapping to. However, as we will see later, specifying a RKHS from scratch is a lot of work. Fortunately, there is a remarkable theorem that makes this easier}:

\begin{theorem}[Moore-Aronszajn theorem]
Given a kernel $K(\cdot, \cdot) : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$, there exists a unique RKHS $\mathcal{H}$ of functions on $\mathcal{X}$ for which $K$ is the reproducing kernel.
\end{theorem}
\begin{proof}
We give a brief sketch of the proof below. For all $x \in \mathcal{X}$, let $K(x, \cdot)$ be the function such that maps an element $x^\prime \in \mathcal{X}$ to $K(x, x^\prime)$. The set of all linear combinations of these functions forms a vector space $\mathcal{H}_0$ where every element $f \in \mathcal{H}_0$ can be expressed as:
%
\begin{equation}
  f(\cdot) = \sum_{i=1}^n a_i K(x_i, \cdot)
\end{equation}
%
where $n \in \mathbb{N}$, $a_i \in \mathbb{R}$ and $x_i \in \mathcal{X}$ are arbitrary.

To turn $\mathcal{H}_0$ into a Hilbert space, we define the inner product between two functions $f(\cdot) = \sum_{i=1}^n a_i K(x_i, \cdot)$ and $g(\cdot) = \sum_{j=1}^m b_j K(y_i, \cdot)$:
%
\begin{equation}
  \innerProd{\sum_{i=1}^n a_i K(x_i, \cdot), \sum_{j=1}^m b_j K(y_i, \cdot)}_{\mathcal{H}_0} = \sum_{i=1}^n \sum_{j=1}^m a_i b_j K(x_i, y_j)
\end{equation}

We can now show the reproducing property:
%
\begin{align}
  \innerProd{f, K(x, \cdot)}_{\mathcal{H}_0} &= \innerProd{\sum_{i=1}^n a_i K(x_i, \cdot), K(x, \cdot)}_{\mathcal{H}_0} \\
                             &= \sum_{i=1}^n a_i K(x_i, x) \\
                             &= f(x)
\end{align}
%
where $K(x, \cdot)$ is the representer of $x$ in $\mathcal{H}$.

Strictly speaking, $\mathcal{H}_0$ is a \defn{pre-RKHS} since it may not be complete. Skipping some details, we can complete $\mathcal{H}_0$ to form a RKHS $\mathcal{H}$ and prove that $K$ is also the reproducing kernel of $\mathcal{H}$.

Finally, we show the uniqueness of $\mathcal{H}$. Let $\mathcal{G}$ be another RKHS with $K$ as its reproducing kernel. Using the reproducing property,
%
\begin{equation}
	\innerProd{K(x, \cdot), K(y, \cdot)}_\mathcal{H} = K(x, y) = \innerProd{K(x, \cdot), K(y, \cdot)}_\mathcal{G}
\end{equation}
%
for any $x, y \in \mathcal{X}$. \todo{by linearity??}
\end{proof}

From this, we can see that when we choose a kernel, we also implicitly choose a corresponding RKHS.

\subsection{Feature maps}
Given a kernel $K$ over $\mathcal{X}$ with its associated RKHS $\mathcal{H}$, we define the map $\phi(x) \defeq K(x, \cdot)$ and then use the reproducing property to write:
\begin{align}
  K(x, x^\prime) &= \innerProd{K(x, \cdot), K(x^\prime, \cdot)}_\mathcal{H} \\
                                   &= \innerProd{\phi(x), \phi(x^\prime)}_\mathcal{H}
\end{align}

We can view the function $\phi$ as an implicit \defn{feature map} from the input space $\mathcal{X}$ to an RKHS $\mathcal{H}$ called the \defn{feature space}. So evaluating a kernel is equivalent to computing the inner product in a Hilbert space without explicitly mapping between input space and feature space. This technique is known as the \defn{kernel trick}.

\section{Kernel Embedding of Distributions}
\todo{We need a way to embed probability distributions in a RKHS}\citet{smola2007hilbert}.

\subsection{Mean map}
Let $X$ be a random variable over $\Omega$ with probability distribution $P(X)$. Also let $K$ be a kernel on $\Omega \times \Omega$. From the Moore-Aronszajn Theorem, we know there exists an unique RKHS $\mathcal{H}$ where $K$ is a reproducing kernel. Furthermore, we can define the implicit feature map $\phi(x) \defeq K(x, \cdot)$ from $\Omega$ to $\mathcal{H}$ so that $K(x, x^\prime) = \innerProd{\phi(x), \phi(x^\prime)}$.

To map $P(X)$ to an element in $\mathcal{H}$, we use the mean map:
%
\begin{definition}[Mean map]
The embedding of $P(X)$ in $\mathcal{H}$ via the \defn{mean map} is given by
%
\begin{equation}
  \mu_{X}\nomenclature{$\mu_X$}{Embedding of $P(X)$ in a RKHS via the mean-map} \defeq \E_X[k(x, \cdot)] = \E_X[\phi(x)]
\end{equation}
\end{definition}
%
For a known distribution $P(X)$ we can compute the mean map as:
%
\begin{equation}
  \mu_{X} = \int_\Omega \phi(x) \mathrm{d}P(x)
\end{equation}
%
If we do not know the true distribution $P(X)$, we can empirically estimate the mean map from samples $\{ x_1, \dotsc, x_n \}$ drawn independently and identically distributed from $P(X)$:
%
\begin{equation}
   \hat{\mu}_{X} \defeq \frac{1}{n} \sum_{i=1}^n \phi(x_i)
\end{equation}

\subsection{Properties}
A natural question to ask is: why do we use this mapping? This section outlines the theoretical properties of the mean map that makes it attractive.

\todo{One property of this mapping is that it is one-to-one for certain choices of $K$ called \defn{characteristic kernels}. Using retains all the statistical features of $P(X)$.}
The mean map has several advantages. Firstly, by the reproducing property of $\mathcal{H}$, the expectation of any function $f \in \mathcal{H}$ can be found by taking its inner product with the kernel embedding:

\begin{equation}
\E_X[f(X)] = \innerProd{f, \mu_X}_\mathcal{H}
\end{equation}

\begin{equation}
  k(x,x') = \exp\left(-\frac{1}{2\sigma^2} \normLL{x-x'}^2 \right)
\end{equation}

An important concept t

\begin{equation}
  C_{XY} = \E_{X, Y}[\phi(X) \otimes \phi(Y)]
\end{equation}

\begin{equation}
  \widehat{C}_{XY} = \frac{1}{n} \sum_{i=1}^n \phi(x_i) \otimes \phi(y_i)
\end{equation}

\begin{equation}
  C_{Y \mid X} = C_{YX} C_{XX}^{-1}
\end{equation}
