\chapter{Background}
This chapter introduces the mathematical background behind this work. The mathematical theory is motivated by the technique of solving a problem by transforming it into another. Suppose we are working with an abstract set $\mathcal{S}$ (e.g. a set of probability distributions). Instead of working with $\mathcal{S}$ directly, we could map elements of $\mathcal{S}$ to another set $\mathcal{S}^\prime$ and work entirely in $\mathcal{S}^\prime$. If the new set is easier to work with, then this approach could be better than working with the original set. All we need is an ``embedding'' to transform elements of $S$ to $S^\prime$.

The \todo{approach?} is \todo{inspired?} by this technique. Instead of working with probability distributions directly, we embed them in a reproducing kernel Hilbert space (RKHS) and work entirely in the Hilbert space instead. Using a Hilbert space has several benefits:
%
\begin{itemize}
  \item Hilbert spaces generalise Euclidean space, so we can apply our geometric intuition to elements of the space.
  \item They have been successfully used in this way for partial differential equations, quantum mechanics, and many other areas \needcite.
  \item They have a relatively simple structure compared to other topological vector spaces \needcite.
\end{itemize}
%
Our discussion begins with vector spaces, which are algebraic structures that we can extend to a Hilbert space. \todo{We then introduce reproducing kernel Hilbert spaces and positive-definite kernels. Finally, we show how probability distributions can be embedded in a RKHS.}

\section{Vector Spaces}
A vector space, loosely speaking, is a set of objects (called \defn{vectors}) that can be added together and multiplied by numbers (called \defn{scalars}). More formally,

\begin{definition}[Vector space]
A \defn{vector space} over a field\footnote{Informally, a field is a set of elements where addition and multiplication of these elements satisfy several axioms such as associativity and closure. An example of a field is the set of real numbers $\mathbb{R}$.} $\mathbb{F}$ consists of a set $\mathcal{V}$, a \defn{vector addition} operation $\oplus$, and a \defn{scalar multiplication} operation $\otimes$. It must obey the following axioms for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$ and $a, b \in \mathbb{F}$:
%
\begin{description}
  \item[Associativity] $\mathbf{u} \oplus (\mathbf{v} \oplus \mathbf{w}) = (\mathbf{u} \oplus \mathbf{v}) \oplus \mathbf{w}$.
  \item[Commutativity] $\mathbf{u} \oplus \mathbf{v} = \mathbf{v} \oplus \mathbf{w}$ and $a \otimes \mathbf{u} = \mathbf{u} \otimes a$.
  \item[Existence of Identity] There exists elements $\mathbf{0} \in \mathcal{V}$ and $1 \in \mathbb{F}$ such that $\mathbf{0} \oplus \mathbf{u} = \mathbf{u}$ and $1 \otimes \mathbf{u} = \mathbf{u}$.
  \item[Existence of Inverse] For every $\mathbf{u}$, there is an element $-\mathbf{u} \in \mathcal{V}$ such that $\mathbf{u} \oplus (-\mathbf{u}) = \mathbf{0}$.
  \item[Distributivity] $a \otimes (\mathbf{u} \oplus \mathbf{v}) = (a \otimes \mathbf{u}) \oplus (a \otimes \mathbf{v})$ and $(a + b) \otimes \mathbf{u} = (a \otimes \mathbf{u}) \oplus (b \cdot \mathbf{v})$
\end{description}
\end{definition}
%
While the canonical example of a vector space is the space of real n-dimensional vectors $\mathbb{R}^n$, we can also define vector spaces of other mathematical objects. For instance, $\mathbb{R}^\mathbb{R}$, the set of functions from $\mathbb{R}$ to $\mathbb{R}$, is a vector space. In this space, vector addition and scalar multiplication can be defined as:
%
\begin{align}
  (f \oplus g)(x) &\defeq f(x) g(x) \\
  (a \otimes f)(x) &\defeq a f(x)
\end{align}
%
for any $f, g \in \mathbb{R}^\mathbb{R}$ and $a \in \mathbb{R}$.

\section{Banach Spaces}
One nice property to have in our vector space is the notion of \defn{completeness}. Intuitively, a complete vector space has no ``holes'' in it. For example, the interval $(0, 1]$ (i.e. the set of real numbers $0 < x \leq 1$) is not complete since $0$ is not in the set, even though the set contains elements arbitrarily close to $0$.

We can capture this intuition about completeness formally. Given a vector space $\mathcal{V}$ over $\mathbb{R}$, let $d : \mathcal{V} \times \mathcal{V} \to \mathbb{R}$ be a \defn{distance function} that satisfies the following axioms for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$:
%
\begin{description}
  \item[Positive-definiteness] $d(\mathbf{u}, \mathbf{v}) \geq 0$ and $d(\mathbf{u}, \mathbf{v}) = 0 \Leftrightarrow \mathbf{u} = \mathbf{v}$.
  \item[Symmetry] $d(\mathbf{u}, \mathbf{v}) = d(\mathbf{v}, \mathbf{u})$.
  \item[Triangular inequality] $d(\mathbf{u}, \mathbf{w}) \leq d(\mathbf{u}, \mathbf{v}) + d(\mathbf{v}, \mathbf{w})$.
\end{description}
%
Given a distance function $d$, a sequence $\langle x_i \rangle_{i=1}^\infty$ is \defn{Cauchy} if, for every real number $\epsilon > 0$, there is a positive integer $N$ such that for all positive integers $m, n \geq N$, we have $d(x_m, x_n) < \epsilon$. In other words, members of the sequence get closer and closer to each other with respect to $d$ until they converge to a limit.

A vector space $\mathcal{V}$ is \defn{complete} if every Cauchy sequence of vectors in $\mathcal{V}$ converges to a limit also in $\mathcal{V}$. Continuing our example for $(0, 1]$, the sequence $x_i = 1 / i$ is Cauchy and converges to $0$, which is not in the interval. Therefore $(0, 1]$ is not complete.

We can now extend a vector space with this notion of completeness:
%
\begin{definition}[Banach space]
A \defn{Banach} space is vector space that is complete with respect to the distance function:
\begin{equation}
  d(\mathbf{u}, \mathbf{v}) = \normLL{\mathbf{u} - \mathbf{v}}
\end{equation}
where $\normLL{\cdot}$ is a function from $\mathcal{V}$ to $\mathbb{R}$ (called the \defn{norm}) that satisfies the following axioms for all $\mathbf{u}, \mathbf{v} \in \mathcal{V}$ and $a \in \mathbb{R}$:
\begin{description}
  \item[Positive-definiteness] $\normLL{\mathbf{u}} \geq 0$ and $\normLL{\mathbf{u}} = 0 \Leftrightarrow \mathbf{u} = \mathbf{0}$.
  \item[Homogeneity] $\normLL{a \mathbf{u}} = \normL{a} \normLL{\mathbf{u}}$.
  \item[Triangle Inequality] $\normLL{\mathbf{u} + \mathbf{v}} \leq \normLL{\mathbf{u}} + \normLL{\mathbf{v}}$.
\end{description}
\end{definition}

An example of a Banach space is $\mathbb{R}^n$. There are actually an infinite number of norms that can be defined on $\mathbb{R}^n$, with the most common being the L2 norm $\normLL{\mathbf{u}}_2 \defeq \sqrt{\sum_{i=1}^n u_i}$ for a vector $\mathbf{u} \in \mathcal{H}$. 

\section{Hilbert Spaces}

\begin{definition}[Hilbert space]
A \defn{Hilbert space} $\mathcal{H}$ over $\mathbb{F}$ is a Banach space endowed with a inner product operation $\innerProd{\cdot, \cdot}_\mathcal{H} : \mathcal{H} \times \mathcal{H} \to \mathbb{F}$ satisfying the following axioms for all $\mathbf{u}, \mathbf{v} \in \mathcal{H}$ and $a \in \mathbb{F}$:
%
\begin{description}
  \item[Linearity] $\innerProd{a \mathbf{v}, \mathbf{w}}_\mathcal{H} = a \innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{H}$ and $\innerProd{\mathbf{u} \oplus \mathbf{v}, \mathbf{w}}_\mathcal{H} = \innerProd{\mathbf{u}, \mathbf{w}}_\mathcal{H} + \innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{H}$.
  \item[Positive-definiteness] $\innerProd{\mathbf{v}, \mathbf{v}}_\mathcal{H} \geq 0$ and $\innerProd{\mathbf{v}, \mathbf{v}}_\mathcal{H} = 0 \Leftrightarrow \mathbf{v} = \mathbf{0}$.
  \item[Ssymmetry] $\innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{H} = \innerProd{\mathbf{w}, \mathbf{v}}_\mathcal{H}$.
\end{description}
%
where the norm for the Banach space is given by:
%
\begin{equation}
  \normLL{\mathbf{u}}_\mathcal{H} = \sqrt{\innerProd{\mathbf{u}, \mathbf{u}}_\mathcal{H}}
\end{equation}
%
\end{definition}

As we mentioned in the start of this chapter, Hilbert spaces are a generalisation of Euclidean space, so it should not be surprising that $\mathbb{R}^n$ is a Hilbert space, with the inner product given by $\innerProd{\mathbf{u}, \mathbf{v}}_\mathcal{H} \defeq \sum_{i=0}^n u_i v_i$ for vectors $\mathbf{u}, \mathbf{v} \in \mathcal{H}$.

\section{Reproducing Kernel Hilbert Spaces}
We define a reproducing kernel Hilbert space (RKHS) to be a Hilbert space of functions whose elements satisfy a useful property\footnote{A more standard and formal definition is to use bounded linear functionals, but our definition does not require familarity with functional analysis. \needcite}:
%
\begin{definition}[Reproducing Kernel Hilbert Space]
A Hilbert space $\mathcal{H}$ of real-valued functions on $\mathcal{X}$ is a \defn{reproducing kernel Hilbert space (RKHS)} if, for every $x \in \mathcal{X}$, there is unique function $K_x \in \mathcal{H}$ (called the \defn{representer} of $x$) with the reproducing property,
\begin{equation}
  f(x) = \innerProd{f, K_x}_\mathcal{H} \quad \forall f \in \mathcal{H}
\end{equation}
\end{definition}

So in a RKHS $\mathcal{H}$, evaluating a function $f \in \mathcal{H}$ at a point $x \in \mathcal{X}$ is equivalent to taking the inner product of $f$ with the representer of $x$. Using this reproducing property, we can then define the \defn{reproducing kernel} of $\mathcal{H}$ as $K(x, y) \defeq K_x(y) = \innerProd{K_x, K_y}_\mathcal{H}$.

\section{Positive Definite Kernels}
The reproducing kernels in the previous section belong to a special class of functions called \defn{positive definite kernels}:
%
\begin{definition}
Let $\mathcal{X}$ be a nonempty set. A symmetric function $K : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \defn{positive definite kernel} (or just a \defn{kernel}) on $\mathcal{X}$ if it satisfies:
\begin{equation}
	\sum_{i,j=1}^n c_i c_j K(x_i, x_j) \geq 0
\end{equation}
for any $n \in \mathbb{N}$, $x_1, \dots, x_n \in \mathcal{X}$, and $c_1, \dots, c_n \in \mathbb{R}$.
\end{definition}
%
Since $\sum_{i,j=1}^n c_i c_j K(x, x^\prime) = \innerProd{\sum_{i=1}^n c_i K_x, \sum_{j=1}^n c_j K_{x^\prime}} \geq 0$ by the reproducing property, we can see that all reproducing kernels are positive definite kernels.

There are many examples of positive definite kernels, such as linear kernels, polynomial kernels, and Gaussian kernels \todo{give explicit formulas}?.

\subsection{Constructing RKHSs}
\todo{For any of this to be useful, we need to specify what sort of RKHS we are mapping to. However, as we will see later, specifying a RKHS from scratch is a lot of work. Fortunately, there is a remarkable theorem that makes this easier}:

\begin{theorem}[Moore-Aronszajn theorem]
Given a kernel $K(\cdot, \cdot) : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$, there exists a unique RKHS $\mathcal{H}$ of functions on $\mathcal{X}$ for which $K$ is the reproducing kernel.
\end{theorem}
%
\begin{proof}
For all $x \in \mathcal{X}$, let $K(x, \cdot)$ be the function such that maps an element $x^\prime \in \mathcal{X}$ to $K(x, x^\prime)$. The set of all linear combinations of these functions forms a vector space $\mathcal{H}_0$ where every element $f \in \mathcal{H}_0$ can be expressed as:
%
\begin{equation}
  f(\cdot) = \sum_{x \in \mathcal{X}} a_i K(x, \cdot)
\end{equation}
%
where $n \in \mathbb{N}$, $a_i \in \mathbb{R}$ and $x_i \in \mathcal{X}$ are arbitrary.

To turn $\mathcal{H}_0$ into a Hilbert space, we first define the inner product between two functions $f$ and $g$:
%
\begin{equation}
  \innerProd{\sum_{i=1}^n a_j K_{x_j}, \sum_{j=1}^m b_i K_{y_i}} = \sum_{i=1}^n \sum_{j=1}^m a_i b_j K(x_j, x_j)
\end{equation}x
%

Since $\mathcal{H}_0$ may not be complete, we complete $\mathcal{H}_0$ to form a Hilbert space $\mathcal{H}$ by adding the limits of all Cauchy sequences in $\mathcal{H}_0$ to the space.

We can now show the reproducing property:
%
\begin{equation}
	\innerProd{f, K_x} = \innerProd{\sum_{}
\end{equation}
%

\end{proof}

From this, we can see that when we choose a kernel, we also implicitly choose a corresponding RKHS.

\subsection{Feature maps}

\begin{align}
  K(\mathbf{x}, \mathbf{x}^\prime) &= \innerProd{K(\mathbf{x}, \cdot), K(\mathbf{x}^\prime, \cdot)}_\mathcal{H} \\
                                   &\vcentcolon= \innerProd{\phi(\mathbf{x}), \phi(\mathbf{x}^\prime)}_\mathcal{H}
\end{align}

We can view the function $\phi$ as an implicit \defn{feature map} from the input space $\mathcal{X}$ to an inner product space $\mathcal{V}$ called the \defn{feature space}. So evaluating a kernel is equivalent to computing the inner product in a Hilbert space without explicitly mapping between input space and feature space. This technique is known as the \defn{kernel trick}.

\begin{definition}[Gram matrix]
Given a kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ and observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$, the $n \times n$ real matrix $K$ with elements
\begin{equation}
  K_{ij} \defeq k(\mathbf{x}_i, \mathbf{x}_j)
\end{equation}
for $1 \leq i, j \leq n$ is called the \defn{Gram matrix} of $k$ with respect to $\mathbf{x}_1, \dotsc, \mathbf{x}_n$.
\end{definition}


\begin{definition}[Positive definite kernel]
Let $\mathcal{X}$ be a non-empty set. A symmetric function $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \defn{positive definite kernel} (or simply a \defn{kernel}) if the Gram matrix of $k$ with respect to all finite sized observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$ ($n \in \mathbb{N}$) is positive semi-definite.
\end{definition}

\begin{theorem}[Mercer's theorem]
If $k$ is a kernel on $\mathcal{X} \times \mathcal{X}$, then it can be written in the form:
\begin{equation}
  k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\phi(\mathbf{x}), \phi(\mathbf{x}^\prime)}_\mathcal{V}
\end{equation}
for some inner product space $\mathcal{V}$.
\end{theorem}


This is significant. Since , kernels 

As an example, consider the kernel $k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\mathbf{x}, \mathbf{x}^\prime}^2$. If we $\mathbf{x}, \mathbf{x}^\prime \in \mathbb{R}^2$, then $k((x_1, x_2)^\intercal, (x_1^\prime, x_2^\prime)^\intercal)$ can be written as

\begin{equation}
\innerProd{(x_1^2, x_2^2, \sqrt{2} x_1 x_2)^\intercal, (x_1^{\prime 2}, x_2^{\prime 2}, \sqrt{2} x_1^\prime x_2^\prime)^\intercal}
\end{equation}

Note that this decomposition is not unique, since we can also write

\begin{equation}
2
\end{equation}

\section{Reproducing Kernel Hilbert Spaces}
However, there is a special feature space that is unique to a given kernel. 

Let $\mathbb{F}$ be the space of functions mapping $\mathcal{X}$ to $\mathbb{R}$. We use the feature map $\phi(x) : \mathcal{X} \to \mathbb{F}$:
\begin{equation}
	\phi(x) = k(x, \cdot)
\end{equation}
It is important to note here that $\phi(x)$ is a \emph{function}. It takes an element $x^\prime \in \mathcal{X}$ and maps it to the real number $k(x, x^\prime)$. In an abuse of notation, we can equivalently write $\phi(x)(x^\prime) = k(x, x^\prime)$.

\begin{equation}
	f(\cdot) = \sum_{i=1}^n \alpha_i k(x_i, \cdot)
\end{equation}

\begin{equation}
	\innerProd{f, g} \defeq \sum_{i=1}^n \sum_{j=1}^{n^\prime} \alpha_i \beta_j k(x_i, x_j^\prime)
\end{equation}

\begin{definition}[Reproducing kernel]
	A kernel $k$ is a \defn{reproducing kernel} of a Hilbert space $\mathcal{H}$ if $\forall f \in \mathcal{H}, f(x) = \innerProd{k(x, \cdot), f}_\mathcal{H}$.
\end{definition}

Using the reproducing property of $\mathcal{H}$, we can express the inner product of two elements in the RKHS $k(x, \cdot)$ and $k(x^\prime, \cdot)$ as we obtain a feature map:

\begin{align}
	\innerProd{k(x, \cdot), k(x^\prime, \cdot)} &= k(x, x^\prime)
\end{align}

We use the feature map $\phi(X)$



\section{Kernel Embedding of Distributions}
Now that we have established the RKHS to map to, we need a way to map or embed probability distributions in it.

Let $X$ be a random variable with codomain $\Omega$ and probability distribution $P(X)$. Also let $k$ be a kernel on $\Omega \times \Omega$. From the Moore-Aronszajn Theorem, we know there exists an unique RKHS $\mathcal{H}$ where $k$ is a reproducing kernel. Furthermore, we can define the implicit feature map $\phi(x) \defeq k(x, \cdot)$ from $\Omega$ to $\mathcal{H}$ so that $k(x, x^\prime) = \innerProd{phi(x), phi(x)^\prime}$.

\begin{definition}[Mean embedding]
The embedding of $P(X)$ in $\mathcal{H}$ via the \defn{mean embedding} is given by
\begin{equation}
  \mu_{X}\nomenclature{$\mu_X$}{Embedding of $X$ in a RKHS via the mean-map} \defeq \E[k(X, \cdot)] = \E[\phi(X)]
\end{equation}
\end{definition}

The mean embedding of a probability distribution transforms it to an element in $\mathcal{H}$.

\begin{equation}
  \mu_{X} = \int_\Omega \phi(x) \mathrm{d}P(x)
\end{equation}

Since we rarely know the true distribution $P(X)$, we can use empirically estimate the mean embedding from samples drawn I.I.D. from $P(X)$:

\begin{equation}
\mu_{X} = \frac{1}{n} \sum_{i=1}^n \phi(x_i)
\end{equation}

This mean embedding has several advantages. Firstly, by the reproducing property of $\mathcal{H}$, the expectation of any function $f \in \mathcal{H}$ can be found by taking its inner product with the kernel embedding:

\begin{equation}
\E_X[f(X)] = \innerProd{f, \mu_X}_\mathcal{H}
\end{equation}

\begin{definition}[Characteristic kernel]
  Let If the mean ifmap of a kernel is injective, then the kernel is called \defn{characteristic}.
\end{definition}

So if the kernel we use is characteristic, then any mean embedding in the RKHS retains all the statistical features of the distribution. In other words, no information is lost from this transformation.

\begin{equation}
  k(x,x') = \exp\left(-\frac{1}{2\sigma^2} \normLL{x-x'}^2 \right)
\end{equation}

An important concept t

\begin{equation}
  C_{XY} = \E_{X, Y}[\phi(X) \otimes \phi(Y)]
\end{equation}

\begin{equation}
  \widehat{C}_{XY} = \frac{1}{n} \sum_{i=1}^n \phi(x_i) \otimes \phi(y_i)
\end{equation}

\begin{equation}
  C_{Y \mid X} = C_{YX} C_{XX}^{-1}
\end{equation}
