\chapter{Background}
This chapter introduces the mathematical background for this work. By way of motivating the mathematical theory, suppose we are working with some abstract set $\mathcal{S}$ (for instance, a set of probability distributions). One possible approach would be to map elements of $S$ to another set $S^\prime$ and work within $S^\prime$ instead.

Compared to working directly with $S$, this has the advantage that any properties, theorems, computational savings, and intuition we have for $S^\prime$ become available to us to reason about our original set $S$. All we need is an ``embedding'' or a ``representation theorem'' to transform elements of $S$ to $S^\prime$.

This approach underpins the theory in this thesis. Instead of directly working with probability distributions, we embed them in a Hilbert space. Mapping to a Hilbert space has several benefits:
%
\begin{itemize}
  \item Hilbert spaces are a generalisation of the Euclidean space, so we can apply our geometric intuition.
  \item They are frequently used in mathematics and physics/powerful.
  \item Partially due to its intuitive basis on Euclidean space, they blah.
\end{itemize}
%
This chapter begins with the definition of a vector space, which is a useful algebraic structure. We then extend this structure to a Hilbert space by adding additional operations. We then define positive-definite kernels in order to ...

\section{Vector Spaces}
We begin with \defn{vector spaces}. A vector space, loosely speaking, is a set of objects (called \defn{vectors}) that can be added together and multiplied by numbers (called \defn{scalars}). More formally,

\begin{definition}[Vector space]
A \defn{vector space} over a field\footnote{Informally, a field is a set of elements where addition and multiplication of these elements satisfy several axioms such as associativity and closure. An example of a field is the set of real numbers $\mathbb{R}$.} $\mathbb{F}$ consists of a set $\mathcal{V}$, a \defn{vector addition} operation $\oplus$, and a \defn{scalar multiplication} operation $\otimes$. It must obey the following axioms for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$ and $a, b \in \mathbb{F}$:
%
\begin{description}
  \item[Associativity] $\mathbf{u} \oplus (\mathbf{v} \oplus \mathbf{w}) = (\mathbf{u} \oplus \mathbf{v}) \oplus \mathbf{w}$
  \item[Commutativity] $\mathbf{u} \oplus \mathbf{v} = \mathbf{v} \oplus \mathbf{w}$ and $a \otimes \mathbf{u} = \mathbf{u} \otimes a$
  \item[Existence of Identity] There exists elements $\mathbf{0} \in \mathcal{V}$ and $1 \in \mathbb{F}$ such that $\mathbf{0} \oplus \mathbf{u} = \mathbf{u}$ and $1 \otimes \mathbf{u} = \mathbf{u}$.
  \item[Existence of Inverse] For every element $\mathbf{u}$, there exists an element $-\mathbf{u} \in \mathcal{V}$ such that $\mathbf{u} \oplus (-\mathbf{u}) = \mathbf{0}$.
  \item[Distributivity] $a \otimes (\mathbf{u} \oplus \mathbf{v}) = (a \otimes \mathbf{u}) \oplus (a \otimes \mathbf{v})$ and $(a + b) \otimes \mathbf{u} = (a \otimes \mathbf{u}) \oplus (b \cdot \mathbf{v})$
\end{description}
\end{definition}
%
While the canonical example of a vector space is the space of real n-dimensional vectors $\mathbb{R}^n$, we can also define vector spaces of other mathematical objects. For instance, $\mathbb{R}^\mathbb{R}$, the set of functions from $\mathbb{R}$ to $\mathbb{R}$, is a vector space. In this space, vector addition and scalar multiplication can be defined as:
%
\begin{align}
  (f \oplus g)(x) &\defeq f(x) g(x) \\
  (a \otimes f)(x) &\defeq a f(x)
\end{align}
%
for $f, g \in \mathbb{R}^\mathbb{R}$ and $a \in \mathbb{R}$.

\section{Banach Spaces}
One nice property to have in our vector space is the notion of \defn{completeness}. Intuitively, a complete vector space is one with no ``holes'' in it. For example, the interval $(0, 1]$ (i.e. the set of real numbers $0 < x \leq 1$) is not complete since there is a hole at $0$, even though we can get arbitrarily close to $0$ without leaving the space.

We can capture this intuition formally using Cauchy sequences. Given a vector space $\mathcal{V}$ over $\mathbb{R}$, let $d : \mathcal{V} \times \mathcal{V} \to \mathbb{R}$ be a function that takes two vectors and computes their ``distance''. This function must satisfy the following axioms for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$:
%
\begin{description}
  \item[Positive-definiteness] $d(\mathbf{u}, \mathbf{v}) \geq 0$ and $d(\mathbf{u}, \mathbf{v}) = 0 \Leftrightarrow \mathbf{u} = \mathbf{v}$.
  \item[Symmetry] $d(\mathbf{u}, \mathbf{v}) = d(\mathbf{v}, \mathbf{u})$.
  \item[Triangular inequality] $d(\mathbf{u}, \mathbf{w}) \leq d(\mathbf{u}, \mathbf{v}) + d(\mathbf{v}, \mathbf{w})$.
\end{description}
%
Equipped with this distance function, we can now define a Cauchy sequence. A sequence $\langle x_i \rangle_{i=0}^\infty$ is \defn{Cauchy} if, for every real number $\epsilon > 0$, there is a positive integer $N$ such that for all positive integers $m, n \geq N$, $d(x_m, x_n) < \epsilon$. In other words, members of the sequence get closer and closer to each other in terms of the distance function until they converge to a limit. A space is \defn{complete} if every Cauchy sequence converges to an element in the space. Continuing our example for $(0, 1]$, the sequence $x_i = \frac{1}{i+1}$ is Cauchy and converges to $0$. However, $0$ is not in that set, so $(0, 1]$ is not complete.

There are many ways to define the distance $d$ \todo{is this true?}, but there is one particular choice of $d$ that is useful? Soemthing something Banach space.
%
\begin{definition}[Norm]
Given a vector space $\mathcal{V}$ over $\mathbb{R}$, a \defn{norm} on $\mathcal{V}$ is a function $\normLL{\cdot} : \mathcal{V} \to \mathbb{R}$ that satisfies the following axioms for all $\mathbf{u}, \mathbf{v} \in \mathcal{V}$ and $a \in \mathbb{R}$:
\begin{description}
  \item[Non-negativity] $\normLL{\mathbf{v}} \geq 0$ and $\normLL{\mathbf{v}} = 0 \Leftrightarrow \mathbf{v} = \mathbf{0}$.
  \item[Homogeneity] $\normLL{a \mathbf{v}} = \normL{a} \normLL{\mathbf{v}}$.
  \item[Positive-definiteness] $\normLL{\mathbf{u} + \mathbf{v}} \leq \normLL{\mathbf{u}} + \normLL{\mathbf{v}}$.
\end{description}
\end{definition}
%
We can define the distance function as $d(\mathbf{u}, \mathbf{v}) \defeq \normLL{\mathbf{u} - \mathbf{v}}$. A vector space with a norm is called a \defn{normed vector space}.
%
\begin{definition}[Banach space]
A Banach space is normed vector space that is complete with respect to the distance function induced by the norm.
\end{definition}

\section{Hilbert Spaces}

\begin{definition}[Hilbert space]
A \defn{Hilbert space} $\mathcal{H}$ is a Banach space endowed with a inner product operation $\innerProd{\cdot, \cdot}_\mathcal{H} : \mathcal{H} \times \mathcal{H} \to \mathbb{R}$ satisfying the following axioms for all $\mathbf{u}, \mathbf{v} \in \mathcal{H}$ and $a \in \mathbb{R}$:
%
\begin{description}
  \item[Linearity] $\innerProd{a \mathbf{v}, \mathbf{w}}_\mathcal{H} = a \innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{H}$ and $\innerProd{\mathbf{u} \oplus \mathbf{v}, \mathbf{w}}_\mathcal{H} = \innerProd{\mathbf{u}, \mathbf{w}}_\mathcal{H} + \innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{H}$.
  \item[Positive-definiteness] $\innerProd{\mathbf{v}, \mathbf{v}}_\mathcal{H} \geq 0$ and $\innerProd{\mathbf{v}, \mathbf{v}}_\mathcal{H} = 0 \Leftrightarrow \mathbf{v} = \mathbf{0}$.
  \item[Conjugate symmetry] $\innerProd{\mathbf{v}, \mathbf{w}}_\mathcal{H} = \innerProd{\mathbf{w}, \mathbf{v}}_\mathcal{H}$.
\end{description}
%
The norm for the Banach space is given by:
%
\begin{equation}
  \normLL{\mathbf{u}, \mathbf{u}}_\mathcal{H} = \sqrt{\innerProd{\mathbf{u}, \mathbf{u}}_\mathcal{H}}
\end{equation}
%
\end{definition}

\section{Reproducing Kernel Hilbert Spaces}


%\begin{definition}[Linear functional]
%Given a vector space $\mathcal{V}$ over a field $\mathbb{F}$, a function $f : \mathcal{V} \to \mathbb{F}$ is a \defn{linear functional} if it satisfies the following conditions for all $\mathbf{u}, \mathbf{v} \in \mathcal{V}$ and $a \in \mathbb{F}$:
%\begin{description}
%  \item[Additivity] f(\mathbf{u} \oplus \mathbf{v}) = f(\mathbf{u}) + f(\mathbf{v})
%  \item[Homogeneity of Degree 1] f(a \otimes \mathbf{u}) = a f(\mathbf{u})
%\end{description}
%\end{definition}

A linear functional is a linear function from a vector space to its field.

\begin{definition}[Evaluation functional]
Given a Hilbert space $\mathcal{H}$ over real-valued functions on a set $\mathcal{X}$, the evaluation functional at a point $x \in \mathcal{X}$ is a linear functional $L_x \in \mathcal{H}$ that simply evaluates its argument at $x$:
\begin{equation}
  L_x(f) = f(x)
\end{equation}

\begin{definition}[Reproducing kernel Hilbert space]
A Hilbert space \mathcal{H} is a reproducing kernel Hilbert space if, for all $x \in \mathcal{X}$, the evaluation functional $L_x$ is bounded:
\begin{equation}
  L_x(f) \leq M \normLL{f}_\mathcal{H} \forall f \in \mathcal{H}
\end{equation}
\end{definition}

This definition, however, is hard to apply in practice, so we leverage 

\begin{theorem}[Riesz representation theorem]
If \mathcal{H} is a RKHS of real-valued functions on $\mathcal{X}$, then for every $x \in \mathcal{X}$, there is an unique\todo{is it?} function $K_x \in \mathcal{H}$ with the reproducing property,
\begin{equation}
  L_x(f) = \innerProd{K_x, f}_\mathcal{H} \forall f \in \mathcal{H}
\end{equation}
\end{theorem}

We can then define the \defn{reproducing kernel} of $\mathcal{H}$ as $K(x, x^\prime) \defeq K_x(x^\prime) = \innerProd{K_x, K_{x^\prime}}_\mathcal{H}$.

\section{Kernels}

\begin{definition}[Gram matrix]
Given a kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ and observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$, the $n \times n$ real matrix $K$ with elements
\begin{equation}
	K_{ij} \defeq k(\mathbf{x}_i, \mathbf{x}_j)
\end{equation}
for $1 \leq i, j \leq n$ is called the \defn{Gram matrix} of $k$ with respect to $\mathbf{x}_1, \dotsc, \mathbf{x}_n$.
\end{definition}

\begin{definition}[Positive semi-definite matrix]
A real symmetric $n \times n$ matrix $K$ satisfying
\begin{equation}
	\sum_{i, j=1}^n c_i c_j K_{ij} \geq 0
\end{equation}
for all $c_i, c_j \in \mathbb{R}$ is called \defn{positive semi-definite}. If the term on the left is strictly positive, then $K$ is called \defn{positive definite}.
\end{definition}

\begin{definition}[Positive definite kernel]
Let $\mathcal{X}$ be a non-empty set. A symmetric function $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \defn{positive definite kernel} (or simply a \defn{kernel}) if the Gram matrix of $k$ with respect to all finite sized observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$ ($n \in \mathbb{N}$) is positive semi-definite.
\end{definition}

\begin{theorem}[Mercer's theorem]
If $k$ is a kernel on $\mathcal{X} \times \mathcal{X}$, then it can be written in the form:
\begin{equation}
  k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\phi(\mathbf{x}), \phi(\mathbf{x}^\prime)}_\mathcal{V}
\end{equation}
for some inner product space $\mathcal{V}$.
\end{theorem}

The function $\phi : \mathcal{X} \to \mathcal{V}$ is an implicit \defn{feature map} from the input space $\mathcal{X}$ to an inner product space $\mathcal{V}$ called the \defn{feature space}. Mercer's theorem shows that kernels can be thought as way of computing the inner product in some feature space without explicitly mapping between input space and feature space. This technique is known as the \defn{kernel trick}, which is commonly used in non-linear support vector machines.

This is significant. Since , kernels 

As an example, consider the kernel $k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\mathbf{x}, \mathbf{x}^\prime}^2$. If we $\mathbf{x}, \mathbf{x}^\prime \in \mathbb{R}^2$, then $k((x_1, x_2)^\intercal, (x_1^\prime, x_2^\prime)^\intercal)$ can be written as

\begin{equation}
\innerProd{(x_1^2, x_2^2, \sqrt{2} x_1 x_2)^\intercal, (x_1^{\prime 2}, x_2^{\prime 2}, \sqrt{2} x_1^\prime x_2^\prime)^\intercal}
\end{equation}

Note that this decomposition is not unique, since we can also write

\begin{equation}
2
\end{equation}

\section{Reproducing Kernel Hilbert Spaces}
However, there is a special feature space that is unique to a given kernel. 

Let $\mathbb{F}$ be the space of functions mapping $\mathcal{X}$ to $\mathbb{R}$. We use the feature map $\phi(x) : \mathcal{X} \to \mathbb{F}$:
\begin{equation}
	\phi(x) = k(x, \cdot)
\end{equation}
It is important to note here that $\phi(x)$ is a \emph{function}. It takes an element $x^\prime \in \mathcal{X}$ and maps it to the real number $k(x, x^\prime)$. In an abuse of notation, we can equivalently write $\phi(x)(x^\prime) = k(x, x^\prime)$.

\begin{equation}
	f(\cdot) = \sum_{i=1}^n \alpha_i k(x_i, \cdot)
\end{equation}

\begin{equation}
	\innerProd{f, g} \defeq \sum_{i=1}^n \sum_{j=1}^{n^\prime} \alpha_i \beta_j k(x_i, x_j^\prime)
\end{equation}

\begin{definition}[Reproducing kernel]
	A kernel $k$ is a \defn{reproducing kernel} of a Hilbert space $\mathcal{H}$ if $\forall f \in \mathcal{H}, f(x) = \innerProd{k(x, \cdot), f}_\mathcal{H}$.
\end{definition}

Using the reproducing property of $\mathcal{H}$, we can express the inner product of two elements in the RKHS $k(x, \cdot)$ and $k(x^\prime, \cdot)$ as we obtain a feature map:

\begin{align}
	\innerProd{k(x, \cdot), k(x^\prime, \cdot)} &= k(x, x^\prime)
\end{align}

We use the feature map $\phi(X)$



\begin{theorem}[Moore-Aronszajn theorem]
Suppose $k(\cdot, \cdot)$ is a symmetric, positive definite kernel on a set $\mathcal{X}$. Then there is a unique Hilbert space $\mathcal{H}$ of functions on $\mathcal{X}$ for which $k$ is a reproducing kernel.
\end{theorem}

\section{Kernel Embedding of Distributions}
Now that we have established the RKHS to map to, we need a way to map or embed probability distributions in it.

Let $X$ be a random variable with codomain $\Omega$ and probability distribution $P(X)$. Also let $k$ be a kernel on $\Omega \times \Omega$. From the Moore-Aronszajn Theorem, we know there exists an unique RKHS $\mathcal{H}$ where $k$ is a reproducing kernel. Furthermore, we can define the implicit feature map $\phi(x) \defeq k(x, \cdot)$ from $\Omega$ to $\mathcal{H}$ so that $k(x, x^\prime) = \innerProd{phi(x), phi(x)^\prime}$.

\begin{definition}[Mean embedding]
The embedding of $P(X)$ in $\mathcal{H}$ via the \defn{mean embedding} is given by
\begin{equation}
  \mu_{X}\nomenclature{$\mu_X$}{Embedding of $X$ in a RKHS via the mean-map} \defeq \E[k(X, \cdot)] = \E[\phi(X)]
\end{equation}
\end{definition}

The mean embedding of a probability distribution transforms it to an element in $\mathcal{H}$.

\begin{equation}
  \mu_{X} = \int_\Omega \phi(x) \mathrm{d}P(x)
\end{equation}

Since we rarely know the true distribution $P(X)$, we can use empirically estimate the mean embedding from samples drawn I.I.D. from $P(X)$:

\begin{equation}
\mu_{X} = \frac{1}{n} \sum_{i=1}^n \phi(x_i)
\end{equation}

This mean embedding has several advantages. Firstly, by the reproducing property of $\mathcal{H}$, the expectation of any function $f \in \mathcal{H}$ can be found by taking its inner product with the kernel embedding:

\begin{equation}
\E_X[f(X)] = \innerProd{f, \mu_X}_\mathcal{H}
\end{equation}

\begin{definition}[Characteristic kernel]
  Let If the mean ifmap of a kernel is injective, then the kernel is called \defn{characteristic}.
\end{definition}

So if the kernel we use is characteristic, then any mean embedding in the RKHS retains all the statistical features of the distribution. In other words, no information is lost from this transformation.

\begin{equation}
  k(x,x') = \exp\left(-\frac{1}{2\sigma^2} \normLL{x-x'}^2 \right)
\end{equation}

An important concept t

\begin{equation}
  C_{XY} = \E_{X, Y}[\phi(X) \otimes \phi(Y)]
\end{equation}

\begin{equation}
  \widehat{C}_{XY} = \frac{1}{n} \sum_{i=1}^n \phi(x_i) \otimes \phi(y_i)
\end{equation}

\begin{equation}
  C_{Y \mid X} = C_{YX} C_{XX}^{-1}
\end{equation}
