\chapter{Background}

\todo{Reword section}
This chapter provides the mathematical background for the rest of this work. We begin by introducing two seemingly unrelated concepts: Hilbert spaces and positive-definite kernels, before establishing a remarkable connection between them through the theory of reproducing kernel Hilbert spaces (RKHS). The chapter concludes with a discussion of how we can embed probability distributions can in these spaces and what benefits this brings.

\todo{Do we begin with fields?}

\section{Vector Spaces}
We begin with \defn{vector spaces}. A vector space, loosely speaking, is a set of objects (called \defn{vectors}) that can be added together and multipied by numbers (called \defn{scalars}). More formally,

\begin{definition}[Vector space]
A \defn{vector space} over a field $\mathcal{F}$ consists of a set $\mathcal{V}$, a \defn{vector addition} operation $+$ and a \defn{scalar multiplication} operation $\cdot$. It must obey the following axioms for any $u, v, w \in \mathcal{V}$ and $a, b \in \mathcal{F}$:
\begin{description}
  \item[Associativity] $u + (v + w) = (u + v) + w$
  \item[Commutativity] $u + v = v + w$ and $a \cdot u = u \cdot a$
  \item[Existence of Identity] There exists elements $0 \in \mathcal{V}$ and $1 \in \mathcal{F}$ such that $0 + u = u$ and $1 \cdot u = u$.
  \item[Existence of Inverse] For every element $u$, there exists an element $-u \in \mathcal{V}$ such that $u + (-u) = 0$.
  \item[Distributivity] $a \cdot (u + v) = (a \cdot u) + (a \cdot v)$ and $(a + b) \cdot u = (a \cdot u) + (b \cdot v)$
\end{enumerate}
\end{definition}

While the canonical example of a vector space is $\mathbb{R}^N$, the space of real n-dimensional vectors, this definition is general enough to define vector spaces of other mathematical objects. \todo{Example using vector space of functions}.

Starting with a vector space, we can add additional structure to create other types of spaces:

\todo{Should we work with inner product spaces or just inner products?}

\begin{definition}[Inner product spaces]
A inner product space is a vector space with an extra operation called the \defn{inner product}: $\innerProd{\cdot, \cdot} : \mathcal{V} \times \mathcal{V} \to \mathcal{F}$ that satisfies the following axioms:
\end{definition}

For example, we can extend the space of Euclidean vectors $\mathbb{R}^N$ with the dot product to form a inner product space:

\begin{equation}
  \left\langle \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix},\begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} \right\rangle := x^\text{T} y = \sum_{i=1}^n x_i y_i
\end{equation}

The inner product can 

\section{Norms}

\section{Complete Vector Spaces}

\section{Hilbert Spaces}

\begin{definition}[Hilbert space]
A Hilbert


\section{Kernels}

\begin{definition}[Gram matrix]
Given a kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ and observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$, the $n \times n$ real matrix $K$ with elements
\begin{equation}
	K_{ij} \defeq k(\mathbf{x}_i, \mathbf{x}_j)
\end{equation}
for $1 \leq i, j \leq n$ is called the \defn{Gram matrix} of $k$ with respect to $\mathbf{x}_1, \dotsc, \mathbf{x}_n$.
\end{definition}

\begin{definition}[Positive semi-definite matrix]
A real symmetric $n \times n$ matrix $K$ satisfying
\begin{equation}
	\sum_{i, j=1}^n c_i c_j K_{ij} \geq 0
\end{equation}
for all $c_i, c_j \in \mathbb{R}$ is called \defn{positive semi-definite}. If the term on the left is strictly positive, then $K$ is called \defn{positive definite}.
\end{definition}

\begin{definition}[Positive definite kernel]
Let $\mathcal{X}$ be a non-empty set. A symmetric function $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \defn{positive definite kernel} (or simply a \defn{kernel}) if the Gram matrix of $k$ with respect to all finite sized observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$ ($n \in \mathbb{N}$) is positive semi-definite.
\end{definition}

\begin{theorem}[Mercer's theorem]
If $k$ is a kernel on $\mathcal{X} \times \mathcal{X}$, then it can be written in the form:
\begin{equation}
  k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\phi(\mathbf{x}), \phi(\mathbf{x}^\prime)}_\mathcal{V}
\end{equation}
for some inner product space $\mathcal{V}$.
\end{theorem}

The function $\phi : \mathcal{X} \to \mathcal{V}$ is an implicit \defn{feature map} from the input space $\mathcal{X}$ to an inner product space $\mathcal{V}$ called the \defn{feature space}. Mercer's theorem shows that kernels can be thought as way of computing the inner product in some feature space without explicitly mapping between input space and feature space. This technique is known as the \defn{kernel trick}, which is commonly used in non-linear support vector machines.

This is significant. Since , kernels 

As an example, consider the kernel $k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\mathbf{x}, \mathbf{x}^\prime}^2$. If we $\mathbf{x}, \mathbf{x}^\prime \in \mathbb{R}^2$, then $k((x_1, x_2)^\intercal, (x_1^\prime, x_2^\prime)^\intercal)$ can be written as

\begin{equation}
\innerProd{(x_1^2, x_2^2, \sqrt{2} x_1 x_2)^\intercal, (x_1^{\prime 2}, x_2^{\prime 2}, \sqrt{2} x_1^\prime x_2^\prime)^\intercal}
\end{equation}

Note that this decomposition is not unique, since we can also write

\begin{equation}
2
\end{equation}

\section{Reproducing Kernel Hilbert Spaces}
However, there is a special feature space that is unique to a given kernel. 

Let $\mathcal{F}$ be the space of functions mapping $\mathcal{X}$ to $\mathbb{R}$. We use the feature map $\phi(x) : \mathcal{X} \to \mathcal{F}$:
\begin{equation}
	\phi(x) = k(x, \cdot)
\end{equation}
It is important to note here that $\phi(x)$ is a \emph{function}. It takes an element $x^\prime \in \mathcal{X}$ and maps it to the real number $k(x, x^\prime)$. In an abuse of notation, we can equivalently write $\phi(x)(x^\prime) = k(x, x^\prime)$.

\begin{equation}
	f(\cdot) = \sum_{i=1}^n \alpha_i k(x_i, \cdot)
\end{equation}

\begin{equation}
	\innerProd{f, g} \defeq \sum_{i=1}^n \sum_{j=1}^{n^\prime} \alpha_i \beta_j k(x_i, x_j^\prime)
\end{equation}

\begin{definition}[Reproducing kernel]
	A kernel $k$ is a \defn{reproducing kernel} of a Hilbert space $\mathcal{H}$ if $\forall f \in \mathcal{H}, f(x) = \innerProd{k(x, \cdot), f}_\mathcal{H}$.
\end{definition}

Using the reproducing property of $\mathcal{H}$, we can express the inner product of two elements in the RKHS $k(x, \cdot)$ and $k(x^\prime, \cdot)$ as we obtain a feature map:

\begin{align}
	\innerProd{k(x, \cdot), k(x^\prime, \cdot)} &= k(x, x^\prime)
\end{align}

We use the feature map $\phi(X)$



\begin{theorem}[Moore-Aronszajn theorem]
Suppose $k(\cdot, \cdot)$ is a symmetric, positive definite kernel on a set $\mathcal{X}$. Then there is a unique Hilbert space $\mathcal{H}$ of functions on $\mathcal{X}$ for which $k$ is a reproducing kernel.
\end{theorem}

Let $X$ be a random variable with codomain $\Omega$ and probability distribution $P(X)$. Also let $k$ be a kernel on $\Omega \times \Omega$. From the Moore-Aronszajn Theorem, we know there exists an unique RKHS $\mathcal{H}$ where $k$ is a reproducing kernel. Furthermore, we can define the implicit feature map $\phi(x) \defeq k(x, \cdot)$ from $\Omega$ to $\mathcal{H}$ so that $k(x, x^\prime) = \innerProd{phi(x), phi(x)^\prime}$.

\section{Kernel Embedding of Distributions}

\begin{definition}[Mean embedding]
The embedding of $P(X)$ in $\mathcal{H}$ via the \defn{mean embedding} is given by
\begin{equation}
  \mu_{X}\nomenclature{$\mu_X$}{Embedding of $X$ in a RKHS via the mean-map} \defeq \E[k(X, \cdot)] = \E[\phi(X)]
\end{equation}
\end{definition}

In other words, the mean embedding is the expected value of the feature map of the kernel under the distribution $P(X)$.

\begin{equation}
\mu_{X} = \int_\Omega \phi(x) \ \mathrm{d}P(x)
\end{equation}

In the common case that we don't know the true distribution, we can use Equation to empirically estimate the mean embedding from samples drawn I.I.D. from $P(X)$.

\begin{equation}
\mu_{X} = \frac{1}{n} \sum_{i=1}^n \phi(x_i)
\end{equation}

The mean embedding has several advantages. Firstly, by the reproducing property of $\mathcal{H}$, the expectation of any function $f \in \mathcal{H}$ can be found by taking its inner product with the kernel embedding:

\begin{equation}
\E_X[f(X)] = \innerProd{f, \mu_X}_\mathcal{H}
\end{equation}

\begin{definition}[Characteristic kernel]
  Let If the mean ifmap of a kernel is injective, then the kernel is called \defn{characteristic}.
\end{definition}

So if the kernel we use is characteristic, then any mean embedding in the RKHS retains all the statistical features of the distribution. In other words, no information is lost from this transformation.

\begin{equation}
  k(x,x') = \exp\left(-\frac{1}{2\sigma^2} \normLL{x-x'}^2 \right)
\end{equation}

An important concept t

\begin{equation}
  C_{XY} = \E_{X, Y}[\phi(X) \otimes \phi(Y)]
\end{equation}

\begin{equation}
	\widehat{C}_{XY} = \frac{1}{n} \sum_{i=1}^n \phi(x_i) \otimes \phi(y_i)
\end{equation}

\begin{equation}
	C_{Y \mid X} = C_{YX} C_{XX}^{-1}
\end{equation}
