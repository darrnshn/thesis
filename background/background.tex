\chapter{Background}

This chapter provides the mathematical background for the rest of this work. We begin by introducing two seemingly unrelated concepts: Hilbert spaces and positive-definite kernels, before establishing a remarkable connection between them through the theory of reproducing kernel Hilbert spaces (RKHS). The chapter concludes with a discussion of we can embed probability distributions can in these spaces and what benefits this technique brings.

\section{Vector Spaces}

\begin{definition}[Vector space]
A \defn{vector space} over a field $\mathcal{F}$ consists of a set $\mathcal{V}$ and two binary operations: \defn{vector addition} (denoted by $\mathbf{u} + \mathbf{v}$ for $\mathbf{u}, \mathbf{v} \in \mathcal{V}$) and \defn{scalar multiplication} (denoted by $\alpha \mathbf{v}$ for $\alpha \in \mathcal{F}, \mathbf{v} \in \mathcal{V}$). These two operations must satisfy the following axioms:

\begin{axiom}[Associativity of addition\nopunct]
	\begin{equation}
		\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}
    \end{equation}
\end{axiom}

\begin{axiom}[Commutativity of addition]
    \begin{equation}
    	\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}
    \end{equation}
\end{axiom}

\end{definition}

\begin{definition}[Inner product space]
A inner product space is a vector space with an extra operation called the \defn{inner product}: $\innerProd{\cdot, \cdot} : \mathcal{V} \times \mathcal{V} \to \mathcal{F}$ that satisfies the following axioms:
\end{definition}

\section{Hilbert Spaces}


\section{Kernels}

\begin{definition}[Gram matrix]
Given a kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ and observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$, the $n \times n$ real matrix $K$ with elements
\begin{equation}
	K_{ij} \defeq k(\mathbf{x}_i, \mathbf{x}_j)
\end{equation}
for $1 \leq i, j \leq n$ is called the \defn{Gram matrix} of $k$ with respect to $\mathbf{x}_1, \dotsc, \mathbf{x}_n$.
\end{definition}

\begin{definition}[Positive semi-definite matrix]
A real symmetric $n \times n$ matrix $K$ satisfying
\begin{equation}
	\sum_{i, j=1}^n c_i c_j K_{ij} \geq 0
\end{equation}
for all $c_i, c_j \in \mathbb{R}$ is called \defn{positive semi-definite}. If the term on the left is strictly positive, then $K$ is called \defn{positive definite}.
\end{definition}

\begin{definition}[Positive definite kernel]
Let $\mathcal{X}$ be a non-empty set. A symmetric function $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \defn{positive definite kernel} (or simply a \defn{kernel}) if the Gram matrix of $k$ with respect to all finite sized observations $\{ \mathbf{x}_1, \dotsc, \mathbf{x}_n \} \in \mathcal{X}$ ($n \in \mathbb{N}$) is positive semi-definite.
\end{definition}

\begin{theorem}[Mercer's theorem]
If $k$ is a kernel on $\mathcal{X} \times \mathcal{X}$, then it can be written in the form:
\begin{equation}
  k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\phi(\mathbf{x}), \phi(\mathbf{x}^\prime)}_\mathcal{V}
\end{equation}
for some inner product space $\mathcal{V}$.
\end{theorem}

The function $\phi : \mathcal{X} \to \mathcal{V}$ is an implicit \defn{feature map} from the input space $\mathcal{X}$ to an inner product space $\mathcal{V}$ called the \defn{feature space}. Mercer's theorem shows that kernels can be thought as way of computing the inner product in some feature space without explicitly mapping between input space and feature space. This technique is known as the \defn{kernel trick}, which is commonly used in non-linear support vector machines.

This is significant. Since , kernels 

As an example, consider the kernel $k(\mathbf{x}, \mathbf{x}^\prime) = \innerProd{\mathbf{x}, \mathbf{x}^\prime}^2$. If we $\mathbf{x}, \mathbf{x}^\prime \in \mathbb{R}^2$, then $k((x_1, x_2)^\intercal, (x_1^\prime, x_2^\prime)^\intercal)$ can be written as

\begin{equation}
\innerProd{(x_1^2, x_2^2, \sqrt{2} x_1 x_2)^\intercal, (x_1^{\prime 2}, x_2^{\prime 2}, \sqrt{2} x_1^\prime x_2^\prime)^\intercal}
\end{equation}

Note that this decomposition is not unique, since we can also write

\begin{equation}
2
\end{equation}

\section{Reproducing Kernel Hilbert Spaces}
However, there is a special feature space that is unique to a given kernel. 

Let $\mathcal{F}$ be the space of functions mapping $\mathcal{X}$ to $\mathbb{R}$. We use the feature map $\phi(x) : \mathcal{X} \to \mathcal{F}$:
\begin{equation}
	\phi(x) = k(x, \cdot)
\end{equation}
It is important to note here that $\phi(x)$ is a \emph{function}. It takes an element $x^\prime \in \mathcal{X}$ and maps it to the real number $k(x, x^\prime)$. In an abuse of notation, we can equivalently write $\phi(x)(x^\prime) = k(x, x^\prime)$.

\begin{equation}
	f(\cdot) = \sum_{i=1}^n \alpha_i k(x_i, \cdot)
\end{equation}

\begin{equation}
	\innerProd{f, g} \defeq \sum_{i=1}^n \sum_{j=1}^{n^\prime} \alpha_i \beta_j k(x_i, x_j^\prime)
\end{equation}

\begin{definition}[Reproducing kernel]
	A kernel $k$ is a \defn{reproducing kernel} of a Hilbert space $\mathcal{H}$ if $\forall f \in \mathcal{H}, f(x) = \innerProd{k(x, \cdot), f}_\mathcal{H}$.
\end{definition}

Using the reproducing property of $\mathcal{H}$, we can express the inner product of two elements in the RKHS $k(x, \cdot)$ and $k(x^\prime, \cdot)$ as we obtain a feature map:

\begin{align}
	\innerProd{k(x, \cdot), k(x^\prime, \cdot)} &= k(x, x^\prime)
\end{align}

We use the feature map $\phi(X)$



\begin{theorem}[Moore-Aronszajn theorem]
Suppose $k(\cdot, \cdot)$ is a symmetric, positive definite kernel on a set $\mathcal{X}$. Then there is a unique Hilbert space $\mathcal{H}$ of functions on $\mathcal{X}$ for which $k$ is a reproducing kernel.
\end{theorem}

Let $X$ be a random variable with codomain $\Omega$ and probability distribution $P(X)$. Also let $k$ be a kernel on $\Omega \times \Omega$. From the Moore-Aronszajn Theorem, we know there exists an unique RKHS $\mathcal{H}$ where $k$ is a reproducing kernel. Furthermore, we can define the implicit feature map $\phi(x) \defeq k(x, \cdot)$ from $\Omega$ to $\mathcal{H}$ so that $k(x, x^\prime) = \innerProd{phi(x), phi(x)^\prime}$.

\section{Kernel Embedding of Distributions}

\begin{definition}[Mean embedding]
The embedding of $P(X)$ in $\mathcal{H}$ via the \defn{mean embedding} is given by
\begin{equation}
  \mu_{X}\nomenclature{$\mu_X$}{Embedding of $X$ in a RKHS via the mean-map} \defeq \E[k(X, \cdot)] = \E[\phi(X)]
\end{equation}
\end{definition}

In other words, the mean embedding is the expected value of the feature map of the kernel under the distribution $P(X)$.

\begin{equation}
\mu_{X} = \int_\Omega \phi(x) \ \mathrm{d}P(x)
\end{equation}

In the common case that we don't know the true distribution, we can use Equation to empirically estimate the mean embedding from samples drawn I.I.D. from $P(X)$.

\begin{equation}
\mu_{X} = \frac{1}{n} \sum_{i=1}^n \phi(x_i)
\end{equation}

The mean embedding has several advantages. Firstly, by the reproducing property of $\mathcal{H}$, the expectation of any function $f \in \mathcal{H}$ can be found by taking its inner product with the kernel embedding:

\begin{equation}
\E_X[f(X)] = \innerProd{f, \mu_X}_\mathcal{H}
\end{equation}

\begin{definition}[Characteristic kernel]
  Let If the mean ifmap of a kernel is injective, then the kernel is called \defn{characteristic}.
\end{definition}

So if the kernel we use is characteristic, then any mean embedding in the RKHS retains all the statistical features of the distribution. In other words, no information is lost from this transformation.

\begin{equation}
  k(x,x') = \exp\left(-\frac{1}{2\sigma^2} \normLL{x-x'}^2 \right)
\end{equation}

An important concept t

\begin{equation}
  C_{XY} = \E_{X, Y}[\phi(X) \otimes \phi(Y)]
\end{equation}

\begin{equation}
	\widehat{C}_{XY} = \frac{1}{n} \sum_{i=1}^n \phi(x_i) \otimes \phi(y_i)
\end{equation}

\begin{equation}
	C_{Y \mid X} = C_{YX} C_{XX}^{-1}
\end{equation}
